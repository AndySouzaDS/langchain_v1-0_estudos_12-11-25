# üìù EXERC√çCIO GUIADO
# Objetivo: Criar uma fun√ß√£o que permite trocar de modelo dinamicamente durante uma conversa, mantendo o hist√≥rico.
## Passo 1: Estrutura b√°sica
from langchain.chat_models import init_chat_model
from dotenv import load_dotenv

load_dotenv()

# Passo 2: Criar um dicion√°rio com 3 modelos dispon√≠veis
MODELOS_DISPONIVEIS = {
    "1": {
        "nome": "Qwen3-32b (Groq)", 
        "string": "groq:qwen/qwen3-32b"
    },
    "2": {
        "nome": "Llama-3.1-8b (Groq)",
        "string": "groq:llama-3.1-8b-instant"
    },
    "3": {
        "nome": "Llama-3.3-70b (Groq)",
        "string": "groq:llama-3.3-70b-versatile"
    }
}

# Passo 3: Criar fun√ß√£o que inicializa modelo baseado em escolha
def get_model(escolha):
    # configurar um modelo default
    if escolha not in MODELOS_DISPONIVEIS:
        escolha == "1"  

    modelo_string = MODELOS_DISPONIVEIS[escolha]["string"]
    return init_chat_model(modelo_string), escolha

# Passo 4: Implementar fun√ß√£o de troca de modelo
def trocar_modelo():
    """Mostra menu e retorna novo modelo"""
    print("\nüîÑ Escolha um modelo:")
    # 4.1. listar modelos do dicion√°rio MODELOS_DISPONIVEIS
    for key, value in MODELOS_DISPONIVEIS.items():
        print(f" {key}. {value['nome']}")

    # 4.2. ler a escolha do usu√°rio
    escolha = input("\nEscolha (1-3): ").strip()

    # 4.3. validar a escolha
    if escolha in MODELOS_DISPONIVEIS:
        # 4.4. Inicializar novo modelo
        model, escolha = get_model(escolha)
        print(f"‚úÖ Trocado para: {MODELOS_DISPONIVEIS[escolha]['nome']}\n")
        # 4.5. Retornar a escolha e o novo modelo inicializado
        return model, escolha
    else:
        print("‚ùå Escolha inv√°lida. Mantendo o modelo atual.\n")
        return None, None
    
# Passo 5: Loop com hist√≥rico persistente
def chat_com_troca():
    """Chat principal com suporte a troca de modelo"""
    
    print("üí¨ Chat Multi-Mod# 5.1. Mensagens de direcionamentoelo Langchain v1.0")
    print("Comandos: /trocar | /sair\n")

    # 5.2. Inicializar o modelo padr√£o
    model, escolha_atual = get_model("1")
    # 5.3. Mensagens de direcionamento
    print(f"Modelo ativo: {MODELOS_DISPONIVEIS[escolha_atual]['nome']}\n")

    # 5.4. Hist√≥rico (compartilhado entre modelos!)
    messages = []

    # 5.5. loop troca de modelos
    while True:
        # Input usu√°rio
        user_input = input("Voc√™: ").strip()

        # Op√ß√£o de manter modelo
        if not user_input:
            continue

        # Op√ß√£o de sair
        if user_input == "/sair":
            print("\nüëã At√© logo!")
            break

        # Comando de troca de modelo
        if user_input == "/trocar":
            novo_model, nova_escolha = trocar_modelo()
            if novo_model:
                model = novo_model
                escolha_atual = nova_escolha
            continue
        
        # Adicionar mensagem do usu√°rio
        messages.append(
            {
                "role": "user",
                "content": user_input
            }
        )

        # Invocar modelo ATUAL com hist√≥rico COMPLETO
        response = model.invoke(messages)

        # Adicionar resposta ao hist√≥rico
        messages.append(
            {
                "role": "assistant",
                "content": response.content
            }
        )

        # Mostrar informa√ß√µes do modelo e conte√∫do da resposta
        print(f"\nü§ñ [{MODELOS_DISPONIVEIS[escolha_atual]['nome']}]:")
        print(response.content)
        print()

# Passo 6: Executar
if __name__ == "__main__":
    chat_com_troca()
