from langchain.chat_models import init_chat_model
from dotenv import load_dotenv

# validando as variÃ¡veis de ambiente
load_dotenv()

# inicializando o modelo
model = init_chat_model("groq:llama-3.3-70b-versatile")

# histÃ³rico de mensagens (comeÃ§a com system message)
messages = [
    {
        "role": "system",
        "content": "VocÃª Ã© um assistente prestativo e amigÃ¡vel especializado em LangChain."
    }
]

# contador de interaÃ§Ãµes
contador = 0

print("ğŸ’¬ Chat com LangChain v1.0")
print("Digite 'sair' para encerrar\n")

while True:
    # ler a entrada do usuÃ¡rio
    user_input = input("VocÃª: ")

    # verificar se o usuÃ¡rio gostaria de sair do chat
    if user_input == "sair":
        print(f"\nğŸ‘‹ Conversa encerrada. Total de mensagens: {contador * 2}")  # (system e user)
        break

    # adicionar a mensagem do usuÃ¡rio ao histÃ³rico
    messages.append(
        {
            "role": "user",
            "content": user_input
        }
    ) 

    # invocar o modelo com todo o histÃ³rico
    response = model.invoke(messages)

    # adicionar a resposta do assistente ao histÃ³rico
    messages.append(
        {
            "role": "assistant",
            "content": response.content
        }
    )

    # incrementar o contador de mensagens
    contador += 1

    # Mostrar resposta
    print(f"\nğŸ¤– Assistente: {response.content}\n")
    print(f"ğŸ“Š Mensagens trocadas: {contador}")
    print("-" * 50 + "\n")

# EXTRA: Mostrar histÃ³rico completo ao final
print("\nğŸ“œ HistÃ³rico completo da conversa:")
for i, msg in enumerate(messages[1:], 1):  # pula a system message
    role_emoji = "ğŸ‘¤" if msg["role"] == "user" else "ğŸ¤–"
    print(f"{i}. {role_emoji} {msg['role']}: {msg['content'][:50]}...")
