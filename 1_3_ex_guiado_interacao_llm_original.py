from dotenv import load_dotenv
from langchain.chat_models import init_chat_model

# Passo 1: Carregar vari√°veis de ambiente do arquivo .env
load_dotenv()

# Passo 2: Inicializar modelo usando string notation (novidade v1.0!)
# *** Nota: essa nota√ß√£o do llm tamb√©m funciona: "groq:llama-3.3-70b-versatile"
model = init_chat_model("groq:llama-3.3-70b-versatile")

# Passo 3: Criar mensagens
# Messages √© sempre uma lista de dicion√°rios
messages = [
    {
        "role": "user",  # role: √© quem est√° falando (user, assistant, system)
        "content": "O que √© LangChain em 2 frases?"  # content: o que est√° sendo dito
    }
]

# Passo 4: Invocar modelo (fazer a requisi√ß√£o)
response = model.invoke(messages)

# Passo 5: Exibir resposta
print("ü§ñ Resposta do LLM:")
print(response.content)

# Passo 6 (EXTRA): Ver estrutura completa da resposta
print("\nüìä Estrutura completa:")
print(f"Tipo: {type(response)}")
print(f"Role: {response.type}")  # Ser√° 'ai' (assistant)
print(f"Conte√∫do: {response.content}")
